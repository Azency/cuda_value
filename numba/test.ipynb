{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Prob import D_Prob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ax_path = \"../data/lc_ax_male.csv\"\n",
    "bx_path = \"../data/lc_bx_male.csv\"\n",
    "kt_path = \"../data/lc_kt_male.csv\"\n",
    "path = [ax_path, bx_path, kt_path]\n",
    "\n",
    "\n",
    "death_Prob = D_Prob()\n",
    "death_Prob.read_abk(path)\n",
    "\n",
    "interval_P_cache = []\n",
    "\n",
    "x0 = 67\n",
    "x_end =92\n",
    "p, q = 1, 1\n",
    "delta_t = 1/p\n",
    "T0 = 2020\n",
    "k= 15\n",
    "\n",
    "deal_span = (x_end - x0) * p\n",
    "for i in range(k, deal_span):\n",
    "        if i == 0:\n",
    "            interval_P_cache.append(death_Prob.unit_live(x0, T0, i+1, p))\n",
    "        else:\n",
    "            interval_P_cache.append(death_Prob.interval_live_P(x0, T0, i, i+1, p))\n",
    "\n",
    "trans_tau_np = np.array(interval_P_cache)#存活概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95227777 0.9458399  0.938519   0.93016787 0.92060485 0.9096251\n",
      " 0.89702214 0.88261673 0.86628806 0.84799892]\n"
     ]
    }
   ],
   "source": [
    "print(trans_tau_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个模块计算P_tau，并将其存放至GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SM数量: 82\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# 方法1：使用 cuda.get_current_device()\n",
    "device = cuda.get_current_device()\n",
    "print(f\"SM数量: {device.MULTIPROCESSOR_COUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max grid dimension (x): 2147483647\n"
     ]
    }
   ],
   "source": [
    "import numba.cuda as cuda\n",
    "device = cuda.get_current_device()\n",
    "print(\"Max grid dimension (x):\", device.MAX_GRID_DIM_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_Ptrans_tau(x0, T0, x_end, p, k):\n",
    "    ax_path = \"../data/lc_ax_male.csv\"\n",
    "    bx_path = \"../data/lc_bx_male.csv\"\n",
    "    kt_path = \"../data/lc_kt_male.csv\"\n",
    "    path = [ax_path, bx_path, kt_path]\n",
    "\n",
    "\n",
    "    death_Prob = D_Prob()\n",
    "    death_Prob.read_abk(path)\n",
    "\n",
    "    interval_P_cache = []\n",
    "\n",
    "    deal_span = (x_end - x0) * p\n",
    "    print(type(x_end))\n",
    "    for i in range(k, deal_span):\n",
    "            if i == 0:\n",
    "                interval_P_cache.append(death_Prob.unit_live(x0, T0, i+1, p))\n",
    "            else:\n",
    "                interval_P_cache.append(death_Prob.interval_live_P(x0, T0, i, i+1, p))\n",
    "    \n",
    "    trans_tau_np = np.array(interval_P_cache)\n",
    "    \n",
    "    return trans_tau_np\n",
    "\n",
    "x0 = 67\n",
    "x_end = 72\n",
    "T = x_end - x0\n",
    "p, q = 1, 1\n",
    "trans_tau_d = gen_Ptrans_tau(x0, 2020, x_end, p, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将trans_tau_d存至GPU中，并且设置为全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主机端全局变量\n",
    "d_trans_tau_d = None\n",
    "\n",
    "def init_global_Ptau(host_array):\n",
    "    global d_trans_tau_d\n",
    "    d_trans_tau_d = cuda.to_device(host_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将V设置为全局变量，随T迭代更新\n",
    "V是四维变量，取值于X，Y，Z，E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = 0.1\n",
    "min_XYZ = 0\n",
    "max_XYZ = 10\n",
    "size_XYZ = 200\n",
    "size_W = 100\n",
    "# 将0-10的区间均匀分成X_size份\n",
    "X = np.linspace(min_XYZ, max_XYZ, size_XYZ, dtype=np.float64)\n",
    "Y = np.linspace(min_XYZ, max_XYZ, size_XYZ, dtype=np.float64)\n",
    "Z = np.linspace(min_XYZ, max_XYZ, size_XYZ, dtype=np.float64)\n",
    "E = np.arange(2, dtype=np.int16) \n",
    "\n",
    "W = np.linspace(0, 1, size_W, dtype=np.float64)\n",
    "\n",
    "def init_global_V(X, Y, Z, E, a1):\n",
    "    host_array = np.zeros((size_XYZ, size_XYZ, size_XYZ, 2), dtype=np.float64)\n",
    "    # 创建网格\n",
    "    X_mesh, Y_mesh, Z_mesh = np.meshgrid(X, Y, Z, indexing='ij')\n",
    "\n",
    "    print(X_mesh.shape)\n",
    "    \n",
    "    # 计算min(Z, Y)\n",
    "    min_ZY = np.minimum(Z_mesh, Y_mesh)\n",
    "    \n",
    "    # 计算Y - a1*(Y - min(Z, Y))\n",
    "    term = Y_mesh - a1 * (Y_mesh - min_ZY)\n",
    "    \n",
    "    # 计算max(X, term)\n",
    "    result = np.maximum(X_mesh, term)\n",
    "    \n",
    "    # 将结果赋值给host_array\n",
    "    host_array[..., 0] = result\n",
    "    host_array[..., 1] = result\n",
    "    \n",
    "    # 转移到GPU\n",
    "    global d_V\n",
    "    d_V = cuda.to_device(host_array)\n",
    "\n",
    "init_global_V(X, Y, Z, E, a1)\n",
    "\n",
    "\n",
    "scale_to_int = float(size_XYZ)/(max_XYZ-min_XYZ)\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def __lookup_V__(d_V, X, Y, Z, E):\n",
    "    X_int = int(math.floor((X - min_XYZ) * scale_to_int))\n",
    "    Y_int = int(math.floor((Y - min_XYZ) * scale_to_int))\n",
    "    Z_int = int(math.floor((Z - min_XYZ) * scale_to_int))\n",
    "    E_int = int(E)\n",
    "    return d_V[X_int, Y_int, Z_int, E_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Motecalo_Expectation():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 创建kernel函数，每个线程计算一个motecalo值\n",
    "@cuda.jit\n",
    "def motecalo_kernel(d_result, d_random01, d_P_tau, d_V, d_args):\n",
    "    idx = cuda.grid(1)\n",
    "    randmon = d_random01[idx]\n",
    "    # [X, Y, Z, E, W, r, delta_t, mu, sigma, l] = d_args\n",
    "    X = d_args[0]\n",
    "    Y = d_args[1]\n",
    "    Z = d_args[2]\n",
    "    E = d_args[3]\n",
    "    r = d_args[4]\n",
    "    delta_t = d_args[5]\n",
    "    mu = d_args[6]\n",
    "    sigma = d_args[7]\n",
    "    l = d_args[8]\n",
    "    a2 = d_args[9]\n",
    "    a3 = d_args[10]\n",
    "    W = d_args[11]\n",
    "\n",
    "    X_tp1 = (X - W) * math.exp( (mu - l - sigma ** 2 / 2) * delta_t + sigma * math.sqrt(delta_t) * randmon)\n",
    "\n",
    "    E_tp1 = E + W\n",
    "    \n",
    "    min_ZYt = min(Z, Y)\n",
    "    if W == 0:\n",
    "        if E == 0:\n",
    "            Y_tp1 = (1 + a2) * max(X, Y)\n",
    "            Z_tp1 = (1 + a2) * max(a3*X, Y)\n",
    "        elif E > 0:\n",
    "            Y_tp1 = max(X, Y)\n",
    "            Z_tp1 = max(a3*X, Y)\n",
    "    elif W >0 :\n",
    "        if W <= min_ZYt:\n",
    "            Y_tp1 = max(X - W , Y - W)\n",
    "            Z_tp1 = max(a3*(X - W), Z)\n",
    "        elif W > min_ZYt:\n",
    "            Y_tp1 = max(X - W, \n",
    "                           min(Y - W, Y / X * (X - W))\n",
    "                           )\n",
    "            Z_tp1 = max(a3*(X - W), \n",
    "                            Z / X * (X - W)\n",
    "                           )\n",
    "            \n",
    "    \n",
    "\n",
    "    V_tp1 = __lookup_V__(d_V, X_tp1, Y_tp1, Z_tp1, E_tp1)\n",
    "    \n",
    "    P_tau_tp1 = d_P_tau[0] # 这个是P(tau=t+1)时刻的值\n",
    "    P_tau_gep_tp1 = d_P_tau[1] # 这个是P(tau>=t+1)时刻的值\n",
    "    \n",
    "    d_result[idx] = math.exp(-r * delta_t) * (P_tau_tp1 * max(X_tp1, Y_tp1) + P_tau_gep_tp1 * V_tp1)\n",
    "\n",
    "\n",
    "# 创建kernel函数，每个线程计算一个motecalo值\n",
    "@cuda.jit\n",
    "def motecalo_kernel2(d_result, rng_states, d_P_tau, d_V, d_args):\n",
    "    idx = cuda.grid(1)\n",
    "\n",
    "    randomn = xoroshiro128p_normal_float32(rng_states, idx)\n",
    "    # [X, Y, Z, E, W, r, delta_t, mu, sigma, l] = d_args\n",
    "    X = d_args[0]\n",
    "    Y = d_args[1]\n",
    "    Z = d_args[2]\n",
    "    E = d_args[3]\n",
    "    r = d_args[4]\n",
    "    delta_t = d_args[5]\n",
    "    mu = d_args[6]\n",
    "    sigma = d_args[7]\n",
    "    l = d_args[8]\n",
    "    a2 = d_args[9]\n",
    "    a3 = d_args[10]\n",
    "    W = d_args[11]\n",
    "\n",
    "    X_tp1 = (X - W) * math.exp( (mu - l - sigma ** 2 / 2) * delta_t + sigma * math.sqrt(delta_t) * randomn)\n",
    "\n",
    "    E_tp1 = E + W\n",
    "    \n",
    "    min_ZYt = min(Z, Y)\n",
    "    if W == 0:\n",
    "        if E == 0:\n",
    "            Y_tp1 = (1 + a2) * max(X, Y)\n",
    "            Z_tp1 = (1 + a2) * max(a3*X, Y)\n",
    "        elif E > 0:\n",
    "            Y_tp1 = max(X, Y)\n",
    "            Z_tp1 = max(a3*X, Y)\n",
    "    elif W >0 :\n",
    "        if W <= min_ZYt:\n",
    "            Y_tp1 = max(X - W , Y - W)\n",
    "            Z_tp1 = max(a3*(X - W), Z)\n",
    "        elif W > min_ZYt:\n",
    "            Y_tp1 = max(X - W, \n",
    "                           min(Y - W, Y / X * (X - W))\n",
    "                           )\n",
    "            Z_tp1 = max(a3*(X - W), \n",
    "                            Z / X * (X - W)\n",
    "                           )\n",
    "            \n",
    "    \n",
    "\n",
    "    V_tp1 = __lookup_V__(d_V, X_tp1, Y_tp1, Z_tp1, E_tp1)\n",
    "    \n",
    "    P_tau_tp1 = d_P_tau[0] # 这个是P(tau=t+1)时刻的值\n",
    "    P_tau_gep_tp1 = d_P_tau[1] # 这个是P(tau>=t+1)时刻的值\n",
    "    \n",
    "    d_result[idx] = math.exp(-r * delta_t) * (P_tau_tp1 * max(X_tp1, Y_tp1) + P_tau_gep_tp1 * V_tp1)\n",
    "\n",
    "@cuda.reduce\n",
    "def sum_reduce(a, b):\n",
    "    return a + b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_W(kernel_args, W, Motecalo_nums, P_tau_t):\n",
    "    kernel_args.append(W)\n",
    "    d_args = cuda.to_device(np.array(kernel_args, dtype=np.float64))\n",
    "\n",
    "\n",
    "    # 使用numpy创建随机数组\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # 使用 rng.normal() 生成标准正态分布的样本\n",
    "    d_random01 = cuda.to_device(rng.normal(0, 1, Motecalo_nums))\n",
    "\n",
    "    d_result = cuda.to_device(np.zeros(Motecalo_nums, dtype=np.float64))\n",
    "\n",
    "    # 将P(tau=t)和P(tau>=t)存入GPU\n",
    "    d_P_tau = cuda.to_device(np.array(P_tau_t, dtype=np.float64))\n",
    "\n",
    "    all_threads = Motecalo_nums\n",
    "    rng_states = create_xoroshiro128p_states(all_threads, seed=1)\n",
    "    block_size = 1024\n",
    "    blocks_per_grid = (all_threads + block_size - 1) // block_size\n",
    "    motecalo_kernel[blocks_per_grid, block_size](d_result, d_random01, d_P_tau, d_V, d_args)\n",
    "\n",
    "    cuda.synchronize()\n",
    "\n",
    "    \n",
    "    avg = sum_reduce(d_result)/Motecalo_nums\n",
    "\n",
    "    # del d_random01\n",
    "    del d_result\n",
    "    del d_P_tau\n",
    "\n",
    "    cuda.synchronize()\n",
    "\n",
    "\n",
    "    return avg\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numba.njit(parallel=True, nogil=True)\n",
    "def pthread_W(kernel_args, all_W, Motecalo_nums, P_tau_t):\n",
    "    V_of_W = []\n",
    "    for i in range(len(all_W)):\n",
    "        avg = kernel_W(kernel_args, W, Motecalo_nums, P_tau_t)\n",
    "        V_of_W.append(avg)\n",
    "\n",
    "    max_V_of_W = max(V_of_W)\n",
    "    return max_V_of_W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Motecalo_nums = 10000\n",
    "# kernel_args = [X, Y, Z, E, W, r, delta_t, mu, sigma, l]\n",
    "X = 0.1\n",
    "Y = 0.1\n",
    "Z = 0.1\n",
    "E = 0\n",
    "W = 0.2\n",
    "\n",
    "a2 = 0.02\n",
    "a3 = 0.1\n",
    "kernel_args = [X, Y, Z, E, r, delta_t, mu, sigma, l, a2, a3]\n",
    "\n",
    "\n",
    "\n",
    "P_tau = [0.9, 0.95]\n",
    "\n",
    "all_W = np.linspace(0, 1, 10, dtype=np.float64)\n",
    "\n",
    "a = pthread_W(kernel_args, all_W, Motecalo_nums, P_tau)\n",
    "print(a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_fin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
